{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA-211: Optimization for Machine Learning TP1\n",
    "\n",
    "### Authors: Gabriele Lorenzo, Aldo Pietromatera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load(\n",
    "    \"./data/data_center_data_matrix.npy\", allow_pickle=True\n",
    ")\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0], 1)), -(M.T * COP_train[:, 3]).T])\n",
    "b = COP_train[:, 3]\n",
    "\n",
    "# Constructing matrices for the test set\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack(\n",
    "    [M_test, np.ones((M_test.shape[0], 1)), -(M_test.T * COP_test[:, 3]).T]\n",
    ")\n",
    "b_test = COP_test[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit this model to the data, we first standardize the data. Indeed, the measurements may have various units (like kWh, degree Celsius, V, etc) and it makes more sense to separate the statistical aspects from these dimensionality considerations. Hence, we consider the matrix $\\tilde{x}$, which is such that each of its columns has mean and standard deviation respectively 0 and 1 over the training set. (Note that we do not use the test set to do this\n",
    "standardization.)\n",
    "We then solve the following least squares problem:\n",
    "\n",
    "$$\\min\\limits_w = \\frac{1}{2}||Aw - b||^2$$\n",
    "\n",
    "where $(Aw)_t = \\tilde{x}(t)^T w_1 + w_0 - y(t) \\times \\tilde{x}(t)^T w_2$ for all $t$ and $b = y(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.1: Show that if $Aw = b$, then $y(t) = \\frac{w_1^T \\tilde{x}(t) + w_0}{w_2^T \\tilde{x}(t) + 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the $t^{th}$ element of $Aw$, as well as the $t^{th}$ element of $b$ are given by:\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "    (Aw)_t = \\tilde{x}(t)^T w_1 + w_0 - y(t) \\times \\tilde{x}(t)^T w_2\n",
    "    \\\\ \n",
    "    b_{t} = y(t)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "Then, it follows that, for every $t$:\n",
    "\n",
    "$$\n",
    "    (Aw)_{t} = b_{t} \\iff\n",
    "    \\tilde{x}(t)^{T} w_{1} + w_{0} - y(t) \\times \\tilde{x}(t)^{T} w_{2} = y(t) \\iff\n",
    "    \\tilde{x}(t)^{T} w_{1} + w_{0} = y(t) \\times \\tilde{x}(t)^{T} w_{2} + y(t) \\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "    y(t) = \\frac{\\tilde{x}(t)^{T} w_{1} + w_{0}}{\\tilde{x}(t)^{T} w_{2} + 1} \\iff\n",
    "    y(t) = \\frac{w_{1}^{T} \\tilde{x}(t) + w_{0}}{w_{2}^{T} \\tilde{x}(t) + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.2: Solve this least squares problem using the function numpy.linalg.lstsq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [-0.00927821  0.08309371 -0.03672704 ...  0.01980595 -0.03057174\n",
      " -0.01188614]\n"
     ]
    }
   ],
   "source": [
    "model = np.linalg.lstsq(A, b, rcond=None)\n",
    "w = model[0]\n",
    "\n",
    "print(\"w = \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.3: Evaluate the quality of the solution found on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 390.44923967619826\n"
     ]
    }
   ],
   "source": [
    "b_pred = A_test @ w\n",
    "error = (0.5 * np.linalg.norm(b_pred - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.4: In order to improve the generalization power of the model, we consider a $l_2$ regularization:\n",
    "\n",
    "$$ \\min\\limits_w = \\frac{1}{2}||Aw - b||^2 + \\frac{\\lambda}{2}||w||^2$$\n",
    "\n",
    "where $\\lambda = 100$. Solve this problem and compare the test mean square error with the unregularized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reg: 150.52741404709445\n",
      "Regularization improved the generalization power of the model.\n"
     ]
    }
   ],
   "source": [
    "lambda_reg = 100\n",
    "w_reg = np.linalg.solve(A.T @ A + lambda_reg * np.eye(A.shape[1]), A.T @ b)\n",
    "\n",
    "b_pred_reg = A_test @ w_reg\n",
    "error_reg = (0.5 * np.linalg.norm(b_pred_reg - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"Error reg: {error_reg}\")\n",
    "\n",
    "if error_reg < error:\n",
    "    print(\"Regularization improved the generalization power of the model.\")\n",
    "else:\n",
    "    print(\"Regularization did not improve the generalization power of the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.5: Calculate the gradient of $f_1 : w \\mapsto \\frac{1}{2}||Aw - b||^2 + \\frac{\\lambda}{2}||w||^2$. Is the function convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the partial derivatives of the function $f_1$ to calculate the gradient. In the end we obtain the following expression:\n",
    "\n",
    "$$\\nabla f_1(w) = A^T(Aw-b) + \\lambda w$$\n",
    "\n",
    "To prove that the function is convex we can calculate the Hessian matrix and check if it is positive semi-definite. The Hessian matrix is the following:\n",
    "\n",
    "$$H = A^TA + \\lambda I$$\n",
    "\n",
    "Since $A^TA$ is positive semi-definite and $\\lambda > 0$, the Hessian matrix is positive semi-definite and the function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.6: Implement gradient descent to minimize $f_1$. What step size are you choosing ? How many iterations are needed to get $w_k$ such that $||\\nabla f(w_k)|| \\leq 1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Norm of grad: 2500173.6332380194\n",
      "Step: 10000 Norm of grad: 2505.6432064027945\n",
      "Step: 20000 Norm of grad: 1497.5319467260147\n",
      "Step: 30000 Norm of grad: 936.1307167901575\n",
      "Step: 40000 Norm of grad: 591.383379192776\n",
      "Step: 50000 Norm of grad: 375.15125347571035\n",
      "Step: 60000 Norm of grad: 238.48827959739617\n",
      "Step: 70000 Norm of grad: 151.8047161269512\n",
      "Step: 80000 Norm of grad: 96.71160113003442\n",
      "Step: 90000 Norm of grad: 61.6513979863582\n",
      "\n",
      "Error gradient descent: 201.83017688992086\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(A, b, lambda_reg, n_iter, verbose=False):\n",
    "    w_gd = np.random.randn(A.shape[1])\n",
    "    # L = Lipschitz constant of the gradient of f1\n",
    "    L = np.linalg.norm(A.T @ A)\n",
    "    step_size = 1.9 / L\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        grad = A.T @ (A @ w_gd - b) + lambda_reg * w_gd\n",
    "        w_gd -= step_size * grad\n",
    "\n",
    "        if i % 10000 == 0 and verbose:\n",
    "            print(\"Step:\", i, \"Norm of grad:\", np.linalg.norm(grad))\n",
    "\n",
    "        if np.linalg.norm(grad) <= 1 and verbose:\n",
    "            print(f\"Number of iterations: {i}\")\n",
    "            print(f\"Norm of grad: {np.linalg.norm(grad)}\")\n",
    "            break\n",
    "\n",
    "    return w_gd\n",
    "\n",
    "w_gd = gradient_descent(A, b, lambda_reg, n_iter=100000, verbose=True)\n",
    "\n",
    "b_pred_gd = A_test @ w_gd\n",
    "error_gd = (0.5 * np.linalg.norm(b_pred_gd - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"\\nError gradient descent: {error_gd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3.7: (Bonus question) Implement the conjugate gradient method. Compare the convergence rate with the previous algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Norm of grad: 16432808723505.006\n",
      "Step: 100 Norm of grad: 9583196686.9514\n",
      "Step: 200 Norm of grad: 400217.9547585781\n",
      "Step: 300 Norm of grad: 330765.7804483274\n",
      "Step: 400 Norm of grad: 1195.1972734033043\n",
      "Step: 500 Norm of grad: 0.7663805134167783\n",
      "Number of iterations: 500\n",
      "Norm of grad: 0.7663805134167783\n",
      "\n",
      "Error conjugate gradient: 150.52741986415504\n"
     ]
    }
   ],
   "source": [
    "def conjugate_gradient_descent(A, b, lambda_reg, n_iter, verbose=False):\n",
    "    w_cgd = np.random.randn(A.shape[1])\n",
    "    r = A.T @ (A @ w_cgd - b) + lambda_reg * w_cgd\n",
    "    p = -r\n",
    "    n_iter = 1000\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        Ap = A.T @ (A @ p) + lambda_reg * p\n",
    "        alpha = (r.T @ r) / (p.T @ Ap)\n",
    "        w_cgd += alpha * p\n",
    "        r_new = r + alpha * Ap\n",
    "        beta = (r_new.T @ r_new) / (r.T @ r)\n",
    "        p = -r_new + beta * p\n",
    "        r = r_new\n",
    "\n",
    "        if i % 100 == 0 and verbose:\n",
    "            print(\"Step:\", i, \"Norm of grad:\", np.linalg.norm(Ap))\n",
    "\n",
    "        if np.linalg.norm(Ap) <= 1 and verbose:\n",
    "            print(f\"Number of iterations: {i}\")\n",
    "            print(f\"Norm of grad: {np.linalg.norm(Ap)}\")\n",
    "            break\n",
    "\n",
    "    return w_cgd\n",
    "\n",
    "w_cgd = conjugate_gradient_descent(A, b, lambda_reg, n_iter=1000, verbose=True)\n",
    "\n",
    "b_pred_cg = A_test @ w_cgd\n",
    "error_cg = (0.5 * np.linalg.norm(b_pred_cg - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"\\nError conjugate gradient: {error_cg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the conjugate gradient method converges way faster than the gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization for a sparse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have seen that at the optimum, the parameter w has many coordinates with small but nonzero values. In order to force most of them to be exactly 0 and thus, to concentrate on the really informative features, we can solve a Lasso problem, that is a least squares problem with $l_1$ regularization:\n",
    "\n",
    "$$ \\min\\limits_w = \\frac{1}{2}||Aw - b||^2 + \\lambda||w||_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.4.1: Write the objective function as $F_2 = f_2 + g_2$ where $f_2$ is differentiable and the proximal operator of $g_2$ is easy to compute. Recall the formula for $prox_{g2}$. Calculate the gradient of $f_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the objective function as follows:\n",
    "\n",
    "$$F_2(w) = \\frac{1}{2}||Aw-b||^2 + \\lambda||w||_1$$\n",
    "\n",
    "where $f_2(w) = \\frac{1}{2}||Aw-b||^2$ and $g_2(w) = \\lambda||w||_1$.\n",
    "\n",
    "The proximal operator of $g_2$ is the soft thresholding operator:\n",
    "\n",
    "$$prox_{g_2}(w) = S_{\\lambda}(w) = sign(w)(|w| - \\lambda)_+$$\n",
    "\n",
    "The gradient of $f_2$ is the following:\n",
    "\n",
    "$$\\nabla f_2(w) = A^T(Aw-b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.4.2: Code the proximal gradient method. Here, we will take $\\lambda = 200$. What stopping test do you suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Norm of grad: 2712904.347589016\n",
      "Step: 10000 Norm of grad: 2788.453168983582\n",
      "Step: 20000 Norm of grad: 3013.759056243655\n",
      "Step: 30000 Norm of grad: 3178.6708483240645\n",
      "Step: 40000 Norm of grad: 2794.321258192863\n",
      "Step: 50000 Norm of grad: 3218.2621328341806\n",
      "Step: 60000 Norm of grad: 3045.7848311755574\n",
      "Step: 70000 Norm of grad: 2564.9777517876264\n",
      "Step: 80000 Norm of grad: 2529.0646762845463\n",
      "Step: 90000 Norm of grad: 2522.5777787701036\n",
      "\n",
      "Error proximal gradient: 0.42026689147543167\n"
     ]
    }
   ],
   "source": [
    "def prox(x, gamma):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - gamma, 0)\n",
    "\n",
    "def proximal_gradient_descent(A, b, lambda_reg, n_iter, verbose=False):\n",
    "    w_pgd = np.random.randn(A.shape[1])\n",
    "    L = np.linalg.norm(A.T @ A)\n",
    "    step_size = 1 / L\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        grad = A.T @ (A @ w_pgd - b)\n",
    "        w_pgd = prox(w_pgd - step_size * grad, step_size * lambda_reg)\n",
    "\n",
    "        if i % 10000 == 0 and verbose:\n",
    "            print(\"Step:\", i, \"Norm of grad:\", np.linalg.norm(grad))\n",
    "\n",
    "    return w_pgd\n",
    "\n",
    "lambda_reg = 200\n",
    "w_pgd = proximal_gradient_descent(A, b, lambda_reg, n_iter=100000, verbose=True)\n",
    "\n",
    "b_pred_pgd = A_test @ w_pgd\n",
    "error_pgd = (0.5 * np.linalg.norm(b_pred_pgd - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"\\nError proximal gradient: {error_pgd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.4.3: We may try to accelerate the algorithm using line search. Compare the speed of the algorithm with fixed step size and with line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Norm of grad: 120927.59130301421\n",
      "Step: 1 Norm of grad: 44664.486632928085\n",
      "Step: 2 Norm of grad: 27796.21080906795\n",
      "Step: 3 Norm of grad: 14892.04010809433\n",
      "Step: 4 Norm of grad: 13365.158804783141\n",
      "Step: 5 Norm of grad: 11899.630023453017\n",
      "Step: 6 Norm of grad: 14252.803448831837\n",
      "Step: 7 Norm of grad: 11722.092585211549\n",
      "Step: 8 Norm of grad: 10974.269260455703\n",
      "Step: 9 Norm of grad: 12398.901036473037\n",
      "Step: 10 Norm of grad: 11588.164521058374\n",
      "Step: 11 Norm of grad: 11188.497080288213\n",
      "Step: 12 Norm of grad: 10778.312935126769\n",
      "Step: 13 Norm of grad: 10328.220427564655\n",
      "Step: 14 Norm of grad: 11434.279775766297\n",
      "Step: 15 Norm of grad: 11351.413323488265\n",
      "Step: 16 Norm of grad: 11273.640398194333\n",
      "\n",
      "Error proximal gradient with line search: 7.365104290324181\n"
     ]
    }
   ],
   "source": [
    "def f(A, b, lambda_reg, w):\n",
    "    return 0.5 * np.linalg.norm(A @ w - b) ** 2 + lambda_reg * np.sum(np.abs(w))\n",
    "\n",
    "def proximal_gradient_descent_ls(A, b, lambda_reg, n_iter, tol, verbose=False):\n",
    "    w_pgd_ls = np.zeros(A.shape[1])\n",
    "    beta = 0.5\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        grad = A.T @ (A @ w_pgd_ls - b)\n",
    "        gamma = 1\n",
    "\n",
    "        while (\n",
    "            f(A, b, lambda_reg, prox(w_pgd_ls - gamma * grad, lambda_reg * gamma))\n",
    "            > f(A, b, lambda_reg, w_pgd_ls)\n",
    "            + np.dot(grad, prox(w_pgd_ls - gamma * grad, lambda_reg * gamma) - w_pgd_ls)\n",
    "            + (1 / (2 * gamma))\n",
    "            * np.linalg.norm(\n",
    "                w_pgd_ls - prox(w_pgd_ls - gamma * grad, lambda_reg * gamma)\n",
    "            )\n",
    "            ** 2\n",
    "        ):\n",
    "            gamma *= beta\n",
    "\n",
    "        w_new = prox(w_pgd_ls - gamma * grad, lambda_reg * gamma)\n",
    "\n",
    "        if np.linalg.norm(w_new - w_pgd_ls) < tol:\n",
    "            break\n",
    "\n",
    "        w_pgd_ls = w_new\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Step:\", i, \"Norm of grad:\", np.linalg.norm(grad))\n",
    "\n",
    "        if np.linalg.norm(grad) <= 1 and verbose:\n",
    "            print(f\"Number of iterations: {i}\")\n",
    "            print(f\"Norm of grad: {np.linalg.norm(grad)}\")\n",
    "            break\n",
    "\n",
    "    return w_pgd_ls\n",
    "\n",
    "\n",
    "w_pgd_ls = proximal_gradient_descent_ls(\n",
    "    A, b, lambda_reg, n_iter=100000, tol=1e-5, verbose=True\n",
    ")\n",
    "\n",
    "b_pred_pgd_ls = A_test @ w_pgd_ls\n",
    "error_pgd_ls = (0.5 * np.linalg.norm(b_pred_pgd_ls - b_test) ** 2) / b_test.shape[0]\n",
    "print(f\"\\nError proximal gradient with line search: {error_pgd_ls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Choice of the regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question when considering a regularized machine learning problem is: what is the best value for the regularization parameter $\\rho$? Its goal is to force the model to choose less complex solutions in order to generalize better.\n",
    "Hence, to evaluate the generalization performance, we are going to split our data into a training set $X_{train}$, $y_{train}$ and a validation set $X_{valid}$, $y_{valid}$.\n",
    "\n",
    "Then, we solve the regression problem using the training set but test its performance on the validation set. Note that the\n",
    "loss function for the validation set is not necessarily the mean square error (but we will keep it as the mean square error here).\n",
    "\n",
    "Gathering everything the problem we are trying to solve is the following bilevel optimization problem:\n",
    "\n",
    "$$\\max\\limits_{\\rho \\geq 0} \\frac{1}{n_{valid}} \\sum_{j=1}^{n_{valid}}f \\lparen w,x_j,y_j \\rparen$$\n",
    "\n",
    "$$\\hat{w}^{\\lparen \\rho \\rparen} \\in \\arg \\min \\frac{1}{n_{train}} \\sum_{i=1}^{n_{train}} f \\lparen w,x_i,y_i \\rparen + \\rho R(w)$$\n",
    "\n",
    "where $f \\lparen w, X_i, y_i \\rparen = \\frac{1}{2}||Aw - b||^2$ and $R(w)$ is either $R(w)=\\frac{1}{2}||w||_2^2$ or $R(w)=||w||_1$.\n",
    "\n",
    "Since this is a complex nonconvex optimization problem, we are going to evaluate the accuracy on a grid of values for $\\rho$, that is $\\rho \\in \\lbrace\\rho_0 a^k : k \\in \\lbrace0, 1, ..., K\\rbrace\\rbrace$ for given $\\rho_0 > 0$,  $0 < a < 1$ and $K$. Then, we select the parameter $\\hat{w}^{\\lparen \\rho \\rparen}$ that has the smallest 0-1 loss on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the $l_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rho (with l2 reg): 50\n",
      "Best error (with l2 reg): 142.84199447243205\n"
     ]
    }
   ],
   "source": [
    "rho_list = [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "best_rho = 0\n",
    "best_error = np.inf\n",
    "\n",
    "for rho in rho_list:\n",
    "    w = np.linalg.solve(A.T @ A + rho * np.eye(A.shape[1]), A.T @ b)\n",
    "    b_pred = A_test @ w\n",
    "    error = (0.5 * np.linalg.norm(b_pred - b_test) ** 2) / b_test.shape[0]\n",
    "\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_rho = rho\n",
    "\n",
    "print(f\"Best rho (with l2 reg): {best_rho}\")\n",
    "print(f\"Best error (with l2 reg): {best_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
